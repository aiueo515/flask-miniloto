"""
æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã‚¯ãƒ©ã‚¹ - Flaskå¯¾å¿œç‰ˆ
æœ¬æ ¼çš„ãªæ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã‚’å®Ÿè¡Œ
"""

import numpy as np
import pandas as pd
import logging
from collections import Counter
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

logger = logging.getLogger(__name__)

class TimeSeriesCrossValidator:
    """æœ¬æ ¼çš„ãªæ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã‚¯ãƒ©ã‚¹ï¼ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»20ã‚»ãƒƒãƒˆäºˆæ¸¬å¯¾å¿œï¼‰"""
    
    def __init__(self, min_train_size=10):
        self.min_train_size = min_train_size
        self.fixed_window_results = {}  # çª“ã‚µã‚¤ã‚ºåˆ¥ã®çµæœ
        self.expanding_window_results = []
        self.validation_history = []
        self.feature_importance_history = {}
        
        # æœ¬ç•ªã¨åŒã˜ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«
        self.validation_models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100, max_depth=12, random_state=42, n_jobs=-1
            ),
            'gradient_boost': GradientBoostingClassifier(
                n_estimators=80, max_depth=8, random_state=42
            ),
            'neural_network': MLPClassifier(
                hidden_layer_sizes=(128, 64, 32), max_iter=300, random_state=42
            )
        }
        
        self.model_weights = {
            'random_forest': 0.4,
            'gradient_boost': 0.35,
            'neural_network': 0.25
        }
        
    def evaluate_prediction_sets(self, predicted_sets, actual):
        """20ã‚»ãƒƒãƒˆäºˆæ¸¬ã¨å®Ÿéš›ã®ä¸€è‡´ã‚’è©•ä¾¡"""
        results = []
        
        actual_set = set(actual)
        
        for i, predicted in enumerate(predicted_sets):
            predicted_set = set([int(x) for x in predicted])
            matches = len(predicted_set & actual_set)
            
            result = {
                'set_idx': i,
                'matches': matches,
                'accuracy': matches / 7.0,
                'predicted': [int(x) for x in predicted],
                'actual': actual,
                'matched_numbers': sorted(list(predicted_set & actual_set)),
                'missed_numbers': sorted(list(actual_set - predicted_set)),
                'extra_numbers': sorted(list(predicted_set - actual_set))
            }
            results.append(result)
        
        # å…¨ä½“çµ±è¨ˆ
        all_matches = [r['matches'] for r in results]
        summary = {
            'avg_matches': np.mean(all_matches),
            'max_matches': max(all_matches),
            'min_matches': min(all_matches),
            'std_matches': np.std(all_matches),
            'sets_3_plus': sum(1 for m in all_matches if m >= 3),
            'sets_4_plus': sum(1 for m in all_matches if m >= 4),
            'sets_5_plus': sum(1 for m in all_matches if m >= 5),
            'match_distribution': dict(Counter(all_matches)),
            'individual_results': results
        }
        
        return summary
    
    def create_validation_features(self, data, main_cols):
        """æœ¬ç•ªã¨åŒã˜16æ¬¡å…ƒãƒ•ãƒ«ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        try:
            features = []
            targets = []
            freq_counter = Counter()
            pair_freq = Counter()
            
            for i in range(len(data)):
                try:
                    current = []
                    for col in main_cols:
                        if col in data.columns:
                            current.append(int(data.iloc[i][col]))
                    
                    if len(current) != 7:
                        continue
                    
                    if not all(1 <= x <= 37 for x in current):
                        continue
                    if len(set(current)) != 7:
                        continue
                    
                    # åŸºæœ¬çµ±è¨ˆ
                    for num in current:
                        freq_counter[num] += 1
                    
                    # ãƒšã‚¢åˆ†æ
                    for j in range(len(current)):
                        for k in range(j+1, len(current)):
                            pair = tuple(sorted([current[j], current[k]]))
                            pair_freq[pair] += 1
                    
                    # æœ¬ç•ªã¨åŒã˜16æ¬¡å…ƒç‰¹å¾´é‡
                    sorted_nums = sorted(current)
                    gaps = [sorted_nums[j+1] - sorted_nums[j] for j in range(6)]
                    
                    feat = [
                        float(np.mean(current)),           # å¹³å‡
                        float(np.std(current)),            # æ¨™æº–åå·®
                        float(np.sum(current)),            # åˆè¨ˆ
                        float(sum(1 for x in current if x % 2 == 1)),  # å¥‡æ•°æ•°
                        float(max(current)),               # æœ€å¤§å€¤
                        float(min(current)),               # æœ€å°å€¤
                        float(np.median(current)),         # ä¸­å¤®å€¤
                        float(max(current) - min(current)), # ç¯„å›²
                        float(len([j for j in range(len(sorted_nums)-1) 
                                 if sorted_nums[j+1] - sorted_nums[j] == 1])), # é€£ç¶šæ•°
                        float(current[0]),                 # ç¬¬1æ•°å­—
                        float(current[3]),                 # ç¬¬4æ•°å­—
                        float(current[6]),                 # ç¬¬7æ•°å­—
                        float(np.mean(gaps)),              # å¹³å‡ã‚®ãƒ£ãƒƒãƒ—
                        float(max(gaps)),                  # æœ€å¤§ã‚®ãƒ£ãƒƒãƒ—
                        float(min(gaps)),                  # æœ€å°ã‚®ãƒ£ãƒƒãƒ—
                        float(len([x for x in current if x <= 12])), # å°æ•°å­—æ•°
                    ]
                    
                    # æ¬¡å›äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                    if i < len(data) - 1:
                        next_nums = []
                        for col in main_cols:
                            if col in data.columns:
                                next_nums.append(int(data.iloc[i+1][col]))
                        
                        if len(next_nums) == 7:
                            for target_num in next_nums:
                                features.append(feat.copy())
                                targets.append(target_num)
                        
                except Exception as e:
                    continue
            
            logger.info(f"ãƒ•ãƒ«ç‰¹å¾´é‡å®Œæˆ: {len(features)}å€‹ï¼ˆ16æ¬¡å…ƒï¼‰")
            return np.array(features), np.array(targets), freq_counter
            
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, Counter()
    
    def train_validation_models(self, train_data, main_cols):
        """æœ¬ç•ªã¨åŒã˜ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        try:
            # æœ¬ç•ªã¨åŒã˜16æ¬¡å…ƒç‰¹å¾´é‡ä½œæˆ
            X, y, freq_counter = self.create_validation_features(train_data, main_cols)
            if X is None or len(X) < 50:  # æœ€ä½é™å¿…è¦ãªãƒ‡ãƒ¼ã‚¿æ•°
                return None
            
            trained_models = {}
            scalers = {}
            
            for name, model in self.validation_models.items():
                try:
                    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # æœ¬ç•ªã¨åŒã˜å­¦ç¿’
                    model_copy = type(model)(**model.get_params())
                    model_copy.fit(X_scaled, y)
                    
                    trained_models[name] = model_copy
                    scalers[name] = scaler
                    
                except Exception as e:
                    logger.warning(f"ãƒ¢ãƒ‡ãƒ« {name} ã®å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return {
                'models': trained_models, 
                'scalers': scalers,
                'freq_counter': freq_counter
            }
            
        except Exception as e:
            logger.error(f"æ¤œè¨¼ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def generate_validation_predictions(self, model_data, freq_counter, count=20):
        """æœ¬ç•ªã¨åŒã˜ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§20ã‚»ãƒƒãƒˆäºˆæ¸¬ã‚’ç”Ÿæˆ"""
        try:
            if not model_data or not model_data['models']:
                return []
            
            trained_models = model_data['models']
            scalers = model_data['scalers']
            
            # æœ¬ç•ªã¨åŒã˜åŸºæº–ç‰¹å¾´é‡ï¼ˆ16æ¬¡å…ƒï¼‰
            base_features = [19.0, 10.0, 133.0, 3.5, 35.0, 5.0, 19.0, 30.0, 1.0, 10.0, 20.0, 30.0, 4.5, 8.0, 2.0, 3.0]
            
            predictions = []
            
            for i in range(count):
                # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’åé›†ï¼ˆæœ¬ç•ªã¨åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
                ensemble_votes = Counter()
                
                for name, model in trained_models.items():
                    try:
                        scaler = scalers[name]
                        X_scaled = scaler.transform([base_features])
                        
                        # è¤‡æ•°å›äºˆæ¸¬ï¼ˆæœ¬ç•ªã¨åŒã˜å›æ•°ï¼‰
                        for _ in range(8):
                            if hasattr(model, 'predict_proba'):
                                proba = model.predict_proba(X_scaled)[0]
                                classes = model.classes_
                                if len(classes) > 0:
                                    selected = np.random.choice(classes, p=proba/proba.sum())
                                    if 1 <= selected <= 37:
                                        weight = self.model_weights.get(name, 0.33)
                                        ensemble_votes[int(selected)] += weight
                            else:
                                pred = model.predict(X_scaled)[0]
                                if 1 <= pred <= 37:
                                    weight = self.model_weights.get(name, 0.33)
                                    ensemble_votes[int(pred)] += weight
                                    
                    except Exception as e:
                        continue
                
                # é »å‡ºæ•°å­—ã¨çµ„ã¿åˆã‚ã›ï¼ˆæœ¬ç•ªã¨åŒã˜ï¼‰
                frequent_nums = [num for num, _ in freq_counter.most_common(15)]
                for num in frequent_nums[:8]:
                    ensemble_votes[num] += 0.1
                
                # ä¸Šä½7å€‹ã‚’é¸æŠ
                top_numbers = [num for num, _ in ensemble_votes.most_common(7)]
                
                # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ è£œå®Œ
                while len(top_numbers) < 7:
                    candidate = np.random.randint(1, 38)
                    if candidate not in top_numbers:
                        top_numbers.append(candidate)
                
                final_pred = sorted([int(x) for x in top_numbers[:7]])
                predictions.append(final_pred)
            
            return predictions
            
        except Exception as e:
            logger.error(f"æ¤œè¨¼ç”¨äºˆæ¸¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fixed_window_validation(self, data, main_cols, round_col, window_sizes=[10, 20, 30]):
        """è¤‡æ•°çª“ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹å›ºå®šçª“æ¤œè¨¼ï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        logger.info(f"=== å›ºå®šçª“æ¤œè¨¼é–‹å§‹ï¼ˆçª“ã‚µã‚¤ã‚º: {window_sizes}å›ï¼‰ ===")
        
        total_rounds = len(data)
        results_by_window = {}
        
        for window_size in window_sizes:
            logger.info(f"ğŸ”„ {window_size}å›åˆ†çª“ã§ã®æ¤œè¨¼é–‹å§‹")
            results = []
            
            # åŠ¹ç‡åŒ–ï¼šå…¨å›ã§ã¯ãªãä¸€å®šé–“éš”ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            max_tests = min(total_rounds - window_size - 1, 50)  # æœ€å¤§50å›ã®ãƒ†ã‚¹ãƒˆã«åˆ¶é™
            step = max(1, (total_rounds - window_size - 1) // max_tests)
            
            logger.info(f"æ¤œè¨¼ç¯„å›²: {max_tests}å›ï¼ˆstep={step}ï¼‰")
            
            for i in range(0, total_rounds - window_size - 1, step):
                if len(results) >= max_tests:
                    break
                    
                # è¨“ç·´ãƒ‡ãƒ¼ã‚¿: iã€œi+window_size-1
                train_start = i
                train_end = i + window_size
                test_idx = train_end
                
                if test_idx >= total_rounds:
                    break
                
                # è¨“ç·´ãƒ‡ãƒ¼ã‚¿å–å¾—
                train_data = data.iloc[train_start:train_end]
                test_round = data.iloc[test_idx][round_col]
                actual_numbers = []
                for col in main_cols:
                    if col in data.columns:
                        actual_numbers.append(int(data.iloc[test_idx][col]))
                
                if len(actual_numbers) == 7:
                    # ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                    model_data = self.train_validation_models(train_data, main_cols)
                    
                    if model_data and model_data['models']:
                        # æœ¬ç•ªã¨åŒã˜20ã‚»ãƒƒãƒˆäºˆæ¸¬ç”Ÿæˆ
                        predicted_sets = self.generate_validation_predictions(
                            model_data, 
                            model_data['freq_counter'], 
                            20
                        )
                        
                        if predicted_sets:
                            # è©³ç´°è©•ä¾¡
                            eval_result = self.evaluate_prediction_sets(predicted_sets, actual_numbers)
                            eval_result['train_range'] = f"ç¬¬{train_start + 1}å›ã€œç¬¬{train_end}å›"
                            eval_result['test_round'] = test_round
                            eval_result['window_size'] = window_size
                            
                            results.append(eval_result)
                
                # é€²æ—è¡¨ç¤º
                if (len(results) + 1) % 10 == 0:
                    if results:
                        avg_matches = np.mean([r['avg_matches'] for r in results])
                        logger.info(f"  é€²æ—: {len(results)}/{max_tests}ä»¶ | å¹³å‡ä¸€è‡´: {avg_matches:.2f}")
            
            results_by_window[window_size] = results
            
            # çª“ã‚µã‚¤ã‚ºåˆ¥ã‚µãƒãƒªãƒ¼
            if results:
                avg_matches = np.mean([r['avg_matches'] for r in results])
                max_matches = max([r['max_matches'] for r in results])
                sets_4_plus = np.mean([r['sets_4_plus'] for r in results])
                logger.info(f"ğŸ“Š {window_size}å›åˆ†çª“ çµæœ:")
                logger.info(f"    æ¤œè¨¼å›æ•°: {len(results)}å› | å¹³å‡ä¸€è‡´: {avg_matches:.3f}å€‹ | æœ€é«˜ä¸€è‡´: {max_matches}å€‹")
        
        self.fixed_window_results = results_by_window
        return results_by_window
    
    def expanding_window_validation(self, data, main_cols, round_col, initial_size=30):
        """ç´¯ç©çª“ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        logger.info(f"=== ç´¯ç©çª“æ¤œè¨¼é–‹å§‹ï¼ˆåˆæœŸã‚µã‚¤ã‚º: {initial_size}å›ï¼‰ ===")
        
        results = []
        total_rounds = len(data)
        
        # åŠ¹ç‡åŒ–ï¼šå…¨å›ã§ã¯ãªãä¸€å®šé–“éš”ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        max_tests = min(total_rounds - initial_size, 30)  # æœ€å¤§30å›ã®ãƒ†ã‚¹ãƒˆã«åˆ¶é™
        step = max(1, (total_rounds - initial_size) // max_tests)
        
        logger.info(f"æ¤œè¨¼ç¯„å›²: {max_tests}å›ï¼ˆstep={step}ï¼‰")
        
        for i in range(0, total_rounds - initial_size, step):
            if len(results) >= max_tests:
                break
                
            test_idx = initial_size + i
            
            if test_idx >= total_rounds:
                break
            
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 0ã€œtest_idx-1ï¼ˆç´¯ç©ï¼‰
            train_data = data.iloc[0:test_idx]
            test_round = data.iloc[test_idx][round_col]
            actual_numbers = []
            for col in main_cols:
                if col in data.columns:
                    actual_numbers.append(int(data.iloc[test_idx][col]))
            
            if len(actual_numbers) == 7:
                # ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                model_data = self.train_validation_models(train_data, main_cols)
                
                if model_data and model_data['models']:
                    # æœ¬ç•ªã¨åŒã˜20ã‚»ãƒƒãƒˆäºˆæ¸¬ç”Ÿæˆ
                    predicted_sets = self.generate_validation_predictions(
                        model_data, 
                        model_data['freq_counter'], 
                        20
                    )
                    
                    if predicted_sets:
                        # è©³ç´°è©•ä¾¡
                        eval_result = self.evaluate_prediction_sets(predicted_sets, actual_numbers)
                        eval_result['train_range'] = f"ç¬¬1å›ã€œç¬¬{test_idx}å›"
                        eval_result['test_round'] = test_round
                        eval_result['train_size'] = len(train_data)
                        
                        results.append(eval_result)
            
            # é€²æ—è¡¨ç¤º
            if (len(results) + 1) % 10 == 0:
                if results:
                    avg_matches = np.mean([r['avg_matches'] for r in results])
                    logger.info(f"  é€²æ—: {len(results)}/{max_tests}ä»¶ | å¹³å‡ä¸€è‡´: {avg_matches:.2f}")
        
        self.expanding_window_results = results
        
        # ç´¯ç©çª“ã‚µãƒãƒªãƒ¼
        if results:
            avg_matches = np.mean([r['avg_matches'] for r in results])
            max_matches = max([r['max_matches'] for r in results])
            sets_4_plus = np.mean([r['sets_4_plus'] for r in results])
            logger.info(f"ğŸ“Š ç´¯ç©çª“ çµæœ:")
            logger.info(f"    æ¤œè¨¼å›æ•°: {len(results)}å› | å¹³å‡ä¸€è‡´: {avg_matches:.3f}å€‹ | æœ€é«˜ä¸€è‡´: {max_matches}å€‹")
        
        return results
    
    def compare_validation_methods(self):
        """å›ºå®šçª“ï¼ˆè¤‡æ•°ã‚µã‚¤ã‚ºï¼‰ã¨ç´¯ç©çª“ã®çµæœã‚’æ¯”è¼ƒ"""
        logger.info("=== æ¤œè¨¼æ‰‹æ³•ã®è©³ç´°æ¯”è¼ƒåˆ†æ ===")
        
        if not self.fixed_window_results or not self.expanding_window_results:
            logger.error("æ¤œè¨¼çµæœãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return None
        
        comparison_results = {}
        
        # å›ºå®šçª“ï¼ˆå„ã‚µã‚¤ã‚ºï¼‰ã®çµ±è¨ˆ
        for window_size, results in self.fixed_window_results.items():
            if results:
                avg_matches_list = [r['avg_matches'] for r in results]
                max_matches_list = [r['max_matches'] for r in results]
                sets_4_plus_list = [r['sets_4_plus'] for r in results]
                sets_5_plus_list = [r['sets_5_plus'] for r in results]
                
                stats = {
                    'method': f'å›ºå®šçª“ï¼ˆ{window_size}å›ï¼‰',
                    'window_size': window_size,
                    'avg_matches': np.mean(avg_matches_list),
                    'std_matches': np.std(avg_matches_list),
                    'max_matches': max(max_matches_list),
                    'avg_sets_4_plus': np.mean(sets_4_plus_list),
                    'avg_sets_5_plus': np.mean(sets_5_plus_list),
                    'total_tests': len(results)
                }
                comparison_results[f'fixed_{window_size}'] = stats
        
        # ç´¯ç©çª“ã®çµ±è¨ˆ
        if self.expanding_window_results:
            avg_matches_list = [r['avg_matches'] for r in self.expanding_window_results]
            max_matches_list = [r['max_matches'] for r in self.expanding_window_results]
            sets_4_plus_list = [r['sets_4_plus'] for r in self.expanding_window_results]
            sets_5_plus_list = [r['sets_5_plus'] for r in self.expanding_window_results]
            
            expanding_stats = {
                'method': 'ç´¯ç©çª“',
                'avg_matches': np.mean(avg_matches_list),
                'std_matches': np.std(avg_matches_list),
                'max_matches': max(max_matches_list),
                'avg_sets_4_plus': np.mean(sets_4_plus_list),
                'avg_sets_5_plus': np.mean(sets_5_plus_list),
                'total_tests': len(self.expanding_window_results)
            }
            comparison_results['expanding'] = expanding_stats
        
        # æœ€é©æ‰‹æ³•ã®æ±ºå®š
        best_method = None
        best_score = 0
        
        for method_key, stats in comparison_results.items():
            # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå¹³å‡ä¸€è‡´æ•° + 4å€‹ä»¥ä¸Šä¸€è‡´ã‚»ãƒƒãƒˆæ•° + 5å€‹ä»¥ä¸Šä¸€è‡´ã‚»ãƒƒãƒˆæ•°ã®é‡ã¿ä»˜ã‘ï¼‰
            score = stats['avg_matches'] + stats['avg_sets_4_plus'] * 0.5 + stats['avg_sets_5_plus'] * 1.0
            
            if score > best_score:
                best_score = score
                best_method = stats['method']
        
        # æ¨å¥¨äº‹é …ã®æ±ºå®š
        recommendation = 'expanding'
        if 'fixed_30' in comparison_results and comparison_results['fixed_30']['avg_matches'] > comparison_results.get('expanding', {}).get('avg_matches', 0):
            recommendation = 'fixed_30'
        elif 'fixed_20' in comparison_results and comparison_results['fixed_20']['avg_matches'] > comparison_results.get('expanding', {}).get('avg_matches', 0):
            recommendation = 'fixed_20'
        elif 'fixed_10' in comparison_results:
            recommendation = 'fixed_10'
        
        return {
            'detailed_results': comparison_results,
            'best_method': best_method,
            'best_score': best_score,
            'recommendation': recommendation,
            'improvement': best_score - min([stats['avg_matches'] for stats in comparison_results.values()]) if comparison_results else 0
        }
    
    def get_validation_summary(self):
        """æ¤œè¨¼çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        summary = {
            'fixed_window_tested': len(self.fixed_window_results),
            'expanding_window_tested': len(self.expanding_window_results) > 0,
            'total_validations': sum(len(results) for results in self.fixed_window_results.values()) + len(self.expanding_window_results)
        }
        
        if self.fixed_window_results:
            all_fixed_results = []
            for results in self.fixed_window_results.values():
                all_fixed_results.extend(results)
            
            if all_fixed_results:
                summary['fixed_window_performance'] = {
                    'avg_matches': np.mean([r['avg_matches'] for r in all_fixed_results]),
                    'max_matches': max([r['max_matches'] for r in all_fixed_results]),
                    'avg_sets_4_plus': np.mean([r['sets_4_plus'] for r in all_fixed_results])
                }
        
        if self.expanding_window_results:
            summary['expanding_window_performance'] = {
                'avg_matches': np.mean([r['avg_matches'] for r in self.expanding_window_results]),
                'max_matches': max([r['max_matches'] for r in self.expanding_window_results]),
                'avg_sets_4_plus': np.mean([r['sets_4_plus'] for r in self.expanding_window_results])
            }
        
        return summary