"""
ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ - ãƒŸãƒ‹ãƒ­ãƒˆå¯¾å¿œå®Œå…¨ç‰ˆ
ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¯¾å¿œã€ãƒ¢ãƒ‡ãƒ«ã‚„å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã‚’ç®¡ç†
"""

import os
import pickle
import pandas as pd
import logging
import shutil
import tempfile
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

class FileManager:
    """ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹ - ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¯¾å¿œå®Œå…¨ç‰ˆ"""
    
    def __init__(self, base_dir=None):
        # ç’°å¢ƒå¤‰æ•°ã§ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã™ã‚‹ã‹åˆ¤å®š
        self.use_local_storage = os.environ.get('USE_LOCAL_STORAGE', 'false').lower() == 'true'
        
        if self.use_local_storage:
            # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆ/tmpä½¿ç”¨ï¼‰
            self.data_dir = os.environ.get('DATA_DIR', '/tmp/miniloto_data')
            logger.info(f"ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ¢ãƒ¼ãƒ‰: {self.data_dir}")
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ°¸ç¶šã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰
            self.data_dir = base_dir if base_dir else './data'
            logger.info(f"ğŸ“ æ°¸ç¶šã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ¢ãƒ¼ãƒ‰: {self.data_dir}")
        
        # åŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.base_dir = self.data_dir
        self.models_dir = os.path.join(self.data_dir, 'models')
        self.cache_dir = os.path.join(self.data_dir, 'cache')
        self.uploads_dir = os.path.join(self.data_dir, 'uploads')
        self.backups_dir = os.path.join(self.data_dir, 'backups')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
        self.model_path = os.path.join(self.models_dir, 'miniloto_model.pkl')
        self.history_path = os.path.join(self.data_dir, 'prediction_history.csv')
        self.data_cache_path = os.path.join(self.cache_dir, 'miniloto_data.csv')
        self.config_path = os.path.join(self.data_dir, 'config.json')
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆæœŸåŒ–
        self._ensure_directories()
        
        # èµ·å‹•æ™‚æƒ…å ±ãƒ­ã‚°
        self._log_storage_info()
    
    def _ensure_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        directories = [
            self.data_dir,
            self.models_dir,
            self.cache_dir,
            self.uploads_dir,
            self.backups_dir
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.debug(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºä¿: {directory}")
    
    def _log_storage_info(self):
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        try:
            storage_info = self.get_storage_info()
            logger.info(f"ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±: {storage_info['storage_type']}")
            logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {storage_info['data_dir']}")
            if storage_info.get('warning'):
                logger.warning(f"âš ï¸ {storage_info['warning']}")
        except Exception as e:
            logger.warning(f"ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_file_path(self, filename, subdirectory=None):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        if subdirectory:
            directory = os.path.join(self.data_dir, subdirectory)
            os.makedirs(directory, exist_ok=True)
            return os.path.join(directory, filename)
        return os.path.join(self.data_dir, filename)
    
    # ===== å­˜åœ¨ç¢ºèªãƒ¡ã‚½ãƒƒãƒ‰ =====
    
    def model_exists(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        return os.path.exists(self.model_path)
    
    def history_exists(self):
        """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        return os.path.exists(self.history_path)
    
    def data_cached(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        return os.path.exists(self.data_cache_path)
    
    def config_exists(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        return os.path.exists(self.config_path)
    
    # ===== ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ =====
    
    def save_model(self, prediction_system):
        """äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.model_exists():
                self._backup_file(self.model_path, 'model_backup')
            
            model_data = {
                'trained_models': prediction_system.trained_models,
                'scalers': prediction_system.scalers,
                'model_weights': prediction_system.model_weights,
                'model_scores': prediction_system.model_scores,
                'freq_counter': prediction_system.freq_counter,
                'pair_freq': prediction_system.pair_freq,
                'pattern_stats': prediction_system.pattern_stats,
                'data_count': prediction_system.data_count,
                'saved_at': datetime.now().isoformat(),
                # ãƒŸãƒ‹ãƒ­ãƒˆå¯¾å¿œã®è­˜åˆ¥å­
                'game_type': 'miniloto',
                'number_range': 31,
                'select_count': 5,
                'version': '1.0.0'
            }
            
            # è‡ªå‹•ç…§åˆå­¦ç¿’ã®æ”¹å–„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚ä¿å­˜
            if hasattr(prediction_system, 'auto_learner') and \
               hasattr(prediction_system.auto_learner, 'improvement_metrics'):
                model_data['improvement_metrics'] = prediction_system.auto_learner.improvement_metrics
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰ç§»å‹•ï¼ˆã‚¢ãƒˆãƒŸãƒƒã‚¯ä¿å­˜ï¼‰
            temp_path = self.model_path + '.tmp'
            with open(temp_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            # ä¿å­˜æˆåŠŸæ™‚ã®ã¿æ­£å¼ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•
            shutil.move(temp_path, self.model_path)
            
            logger.info(f"âœ… ãƒŸãƒ‹ãƒ­ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {self.model_path}")
            logger.info(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {prediction_system.data_count}")
            logger.info(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ•°: {len(prediction_system.trained_models)}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            temp_path = self.model_path + '.tmp'
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
    
    def load_model(self, prediction_system):
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã«èª­ã¿è¾¼ã¿"""
        try:
            if not self.model_exists():
                logger.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            with open(self.model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
            game_type = model_data.get('game_type', 'unknown')
            if game_type == 'loto7':
                logger.warning("âš ï¸ ãƒ­ãƒˆ7ç”¨ãƒ¢ãƒ‡ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒŸãƒ‹ãƒ­ãƒˆç”¨ã«æ–°è¦å­¦ç¿’ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚")
            elif game_type != 'miniloto':
                logger.warning(f"âš ï¸ æœªçŸ¥ã®ã‚²ãƒ¼ãƒ ã‚¿ã‚¤ãƒ—: {game_type}")
            
            # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã«ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
            prediction_system.trained_models = model_data['trained_models']
            prediction_system.scalers = model_data['scalers']
            prediction_system.model_weights = model_data['model_weights']
            prediction_system.model_scores = model_data['model_scores']
            prediction_system.freq_counter = model_data['freq_counter']
            prediction_system.pair_freq = model_data['pair_freq']
            prediction_system.pattern_stats = model_data['pattern_stats']
            prediction_system.data_count = model_data['data_count']
            
            # æ”¹å–„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¾©å…ƒ
            if 'improvement_metrics' in model_data:
                if hasattr(prediction_system, 'auto_learner'):
                    prediction_system.auto_learner.improvement_metrics = model_data['improvement_metrics']
            
            saved_at = model_data.get('saved_at', 'ä¸æ˜')
            logger.info(f"âœ… ãƒŸãƒ‹ãƒ­ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {self.model_path}")
            logger.info(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {prediction_system.data_count}")
            logger.info(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ•°: {len(prediction_system.trained_models)}")
            logger.info(f"ğŸ•’ ä¿å­˜æ—¥æ™‚: {saved_at}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    # ===== å±¥æ­´ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ =====
    
    def save_history(self, prediction_history):
        """äºˆæ¸¬å±¥æ­´ã‚’CSVã«ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            if not prediction_history.predictions:
                logger.info("ä¿å­˜ã™ã‚‹äºˆæ¸¬å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # æ—¢å­˜å±¥æ­´ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.history_exists():
                self._backup_file(self.history_path, 'history_backup')
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
            rows = []
            for entry in prediction_history.predictions:
                base_row = {
                    'round': entry['round'],
                    'date': entry['date'],
                    'verified': entry['verified'],
                    'created_at': entry.get('created_at', ''),
                    'game_type': 'miniloto'
                }
                
                # å„äºˆæ¸¬ã‚»ãƒƒãƒˆã‚’è¡Œã¨ã—ã¦è¿½åŠ ï¼ˆãƒŸãƒ‹ãƒ­ãƒˆã¯5å€‹ï¼‰
                for i, pred_set in enumerate(entry['predictions']):
                    row = base_row.copy()
                    row['prediction_idx'] = i
                    for j in range(5):  # ãƒŸãƒ‹ãƒ­ãƒˆã¯5å€‹
                        row[f'pred_{j+1}'] = pred_set[j] if j < len(pred_set) else None
                    
                    # æ¤œè¨¼æ¸ˆã¿ã®å ´åˆã¯å®Ÿéš›ã®ç•ªå·ã¨ä¸€è‡´æ•°ã‚‚è¨˜éŒ²
                    if entry['verified'] and entry.get('actual'):
                        for j in range(5):  # ãƒŸãƒ‹ãƒ­ãƒˆã¯5å€‹
                            if j < len(entry['actual']):
                                row[f'actual_{j+1}'] = entry['actual'][j]
                        if 'matches' in entry and i < len(entry['matches']):
                            row['matches'] = entry['matches'][i]
                    
                    rows.append(row)
            
            df = pd.DataFrame(rows)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰ç§»å‹•
            temp_path = self.history_path + '.tmp'
            df.to_csv(temp_path, index=False, encoding='utf-8')
            shutil.move(temp_path, self.history_path)
            
            logger.info(f"âœ… ãƒŸãƒ‹ãƒ­ãƒˆäºˆæ¸¬å±¥æ­´ã‚’CSVã«ä¿å­˜: {self.history_path}")
            logger.info(f"ğŸ“ ä¿å­˜ä»¶æ•°: {len(rows)}è¡Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            temp_path = self.history_path + '.tmp'
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
    
    def load_history(self, prediction_history):
        """CSVã‹ã‚‰äºˆæ¸¬å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if not self.history_exists():
                logger.info("äºˆæ¸¬å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            df = pd.read_csv(self.history_path, encoding='utf-8')
            
            if df.empty:
                logger.info("äºˆæ¸¬å±¥æ­´ã¯ç©ºã§ã™")
                return True
            
            # å±¥æ­´ã‚’å†æ§‹ç¯‰
            prediction_history.predictions = []
            
            # round ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            for round_num in df['round'].unique():
                round_data = df[df['round'] == round_num]
                
                if round_data.empty:
                    continue
                
                first_row = round_data.iloc[0]
                
                # äºˆæ¸¬ã‚»ãƒƒãƒˆã‚’åé›†
                predictions = []
                for _, row in round_data.iterrows():
                    pred_set = []
                    for j in range(5):  # ãƒŸãƒ‹ãƒ­ãƒˆã¯5å€‹
                        val = row.get(f'pred_{j+1}')
                        if pd.notna(val):
                            pred_set.append(int(val))
                    
                    if len(pred_set) == 5:
                        predictions.append(pred_set)
                
                # ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
                entry = {
                    'round': int(first_row['round']),
                    'date': first_row['date'],
                    'predictions': predictions,
                    'verified': bool(first_row['verified']),
                    'created_at': first_row.get('created_at', '')
                }
                
                # æ¤œè¨¼æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
                if entry['verified']:
                    actual = []
                    matches = []
                    
                    for j in range(5):  # ãƒŸãƒ‹ãƒ­ãƒˆã¯5å€‹
                        val = first_row.get(f'actual_{j+1}')
                        if pd.notna(val):
                            actual.append(int(val))
                    
                    if len(actual) == 5:
                        entry['actual'] = actual
                        
                        # å„äºˆæ¸¬ã‚»ãƒƒãƒˆã®ä¸€è‡´æ•°ã‚’è¨ˆç®—
                        for pred_set in predictions:
                            match_count = len(set(pred_set) & set(actual))
                            matches.append(match_count)
                        
                        entry['matches'] = matches
                
                prediction_history.predictions.append(entry)
            
            logger.info(f"âœ… ãƒŸãƒ‹ãƒ­ãƒˆäºˆæ¸¬å±¥æ­´ã‚’CSVã‹ã‚‰èª­ã¿è¾¼ã¿: {len(prediction_history.predictions)}ãƒ©ã‚¦ãƒ³ãƒ‰")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    # ===== ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ =====
    
    def save_data_cache(self, data):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            if data is None or len(data) == 0:
                logger.warning("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.data_cached():
                self._backup_file(self.data_cache_path, 'cache_backup')
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰ç§»å‹•
            temp_path = self.data_cache_path + '.tmp'
            data.to_csv(temp_path, index=False, encoding='utf-8')
            shutil.move(temp_path, self.data_cache_path)
            
            logger.info(f"âœ… ãƒŸãƒ‹ãƒ­ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {self.data_cache_path}")
            logger.info(f"ğŸ“Š ä¿å­˜ä»¶æ•°: {len(data)}ä»¶")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            temp_path = self.data_cache_path + '.tmp'
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
    
    def load_data_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if not self.data_cached():
                logger.info("ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return None
            
            df = pd.read_csv(self.data_cache_path, encoding='utf-8')
            logger.info(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒŸãƒ‹ãƒ­ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {len(df)}ä»¶")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    # ===== è¨­å®šç®¡ç† =====
    
    def save_config(self, config_data):
        """è¨­å®šã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            import json
            
            # æ—¢å­˜è¨­å®šã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.config_exists():
                self._backup_file(self.config_path, 'config_backup')
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¿½åŠ 
            config_data['saved_at'] = datetime.now().isoformat()
            config_data['game_type'] = 'miniloto'
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰ç§»å‹•
            temp_path = self.config_path + '.tmp'
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            shutil.move(temp_path, self.config_path)
            
            logger.info(f"âœ… è¨­å®šã‚’ä¿å­˜: {self.config_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            temp_path = self.config_path + '.tmp'
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
    
    def load_config(self):
        """è¨­å®šã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            import json
            
            if not self.config_exists():
                logger.info("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return {}
            
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            logger.info(f"âœ… è¨­å®šã‚’èª­ã¿è¾¼ã¿: {self.config_path}")
            return config_data
            
        except Exception as e:
            logger.error(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    # ===== ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ»ç®¡ç† =====
    
    def get_file_info(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—"""
        file_path = self.get_file_path(filename)
        
        if not os.path.exists(file_path):
            return {
                'exists': False,
                'path': file_path
            }
        
        try:
            stat = os.stat(file_path)
            return {
                'exists': True,
                'size': stat.st_size,
                'size_mb': round(stat.st_size / 1024 / 1024, 2),
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'path': file_path,
                'readable': os.access(file_path, os.R_OK),
                'writable': os.access(file_path, os.W_OK)
            }
        except Exception as e:
            return {
                'exists': True,
                'path': file_path,
                'error': str(e)
            }
    
    def get_storage_info(self):
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—"""
        try:
            total, used, free = shutil.disk_usage(self.data_dir)
            
            file_counts = self._count_files()
            
            return {
                'storage_type': 'local' if self.use_local_storage else 'persistent',
                'data_dir': self.data_dir,
                'total_mb': round(total / 1024 / 1024, 1),
                'used_mb': round(used / 1024 / 1024, 1),
                'free_mb': round(free / 1024 / 1024, 1),
                'file_counts': file_counts,
                'warning': 'ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼šå†èµ·å‹•ã§æ¶ˆå¤±' if self.use_local_storage else None,
                'directories': {
                    'models': self.models_dir,
                    'cache': self.cache_dir,
                    'uploads': self.uploads_dir,
                    'backups': self.backups_dir
                }
            }
        except Exception as e:
            return {
                'storage_type': 'local' if self.use_local_storage else 'persistent',
                'data_dir': self.data_dir,
                'error': str(e)
            }
    
    def _count_files(self):
        """å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = {}
        directories = {
            'total': self.data_dir,
            'models': self.models_dir,
            'cache': self.cache_dir,
            'uploads': self.uploads_dir,
            'backups': self.backups_dir
        }
        
        for name, directory in directories.items():
            try:
                if os.path.exists(directory):
                    counts[name] = len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])
                else:
                    counts[name] = 0
            except Exception:
                counts[name] = 0
        
        return counts
    
    def _backup_file(self, file_path, backup_prefix):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        try:
            if not os.path.exists(file_path):
                return False
            
            filename = os.path.basename(file_path)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_filename = f"{backup_prefix}_{timestamp}_{filename}"
            backup_path = os.path.join(self.backups_dir, backup_filename)
            
            shutil.copy2(file_path, backup_path)
            logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
            
            return True
        except Exception as e:
            logger.warning(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def cleanup_old_files(self, days=30):
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            import time
            current_time = time.time()
            cutoff_time = current_time - (days * 24 * 60 * 60)
            
            cleanup_count = 0
            cleanup_size = 0
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(self.backups_dir):
                for file in os.listdir(self.backups_dir):
                    file_path = os.path.join(self.backups_dir, file)
                    if os.path.isfile(file_path):
                        if os.path.getmtime(file_path) < cutoff_time:
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleanup_count += 1
                            cleanup_size += file_size
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            for root, dirs, files in os.walk(self.data_dir):
                for file in files:
                    if file.endswith('.tmp') or file.endswith('.bak'):
                        file_path = os.path.join(root, file)
                        try:
                            if os.path.getmtime(file_path) < cutoff_time:
                                file_size = os.path.getsize(file_path)
                                os.remove(file_path)
                                cleanup_count += 1
                                cleanup_size += file_size
                        except Exception:
                            pass
            
            if cleanup_count > 0:
                cleanup_size_mb = round(cleanup_size / 1024 / 1024, 2)
                logger.info(f"ğŸ§¹ å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’{cleanup_count}å€‹å‰Šé™¤ã—ã¾ã—ãŸï¼ˆ{cleanup_size_mb}MBè§£æ”¾ï¼‰")
            
            return {
                'files_deleted': cleanup_count,
                'space_freed_mb': round(cleanup_size / 1024 / 1024, 2)
            }
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'files_deleted': 0,
                'space_freed_mb': 0,
                'error': str(e)
            }
    
    # ===== ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ”¯æ´ =====
    
    def get_upload_path(self, filename):
        """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return os.path.join(self.uploads_dir, filename)
    
    def save_uploaded_file(self, file_obj, filename):
        """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        try:
            upload_path = self.get_upload_path(filename)
            file_obj.save(upload_path)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
            file_info = self.get_file_info(os.path.join('uploads', filename))
            
            logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {upload_path}")
            logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_info.get('size_mb', 0)}MB")
            
            return upload_path
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_download_info(self, file_type):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—"""
        file_paths = {
            'model': self.model_path,
            'history': self.history_path,
            'data': self.data_cache_path,
            'config': self.config_path
        }
        
        if file_type not in file_paths:
            return None
        
        file_path = file_paths[file_type]
        if not os.path.exists(file_path):
            return None
        
        return {
            'path': file_path,
            'filename': os.path.basename(file_path),
            'info': self.get_file_info(os.path.basename(file_path))
        }
    
    def export_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨æƒ…å ±ã‚’å–å¾—"""
        export_info = {
            'timestamp': datetime.now().isoformat(),
            'game_type': 'miniloto',
            'storage_type': 'local' if self.use_local_storage else 'persistent',
            'files': {}
        }
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’åé›†
        file_types = ['model', 'history', 'data', 'config']
        for file_type in file_types:
            download_info = self.get_download_info(file_type)
            if download_info:
                export_info['files'][file_type] = {
                    'available': True,
                    'filename': download_info['filename'],
                    'size_mb': download_info['info'].get('size_mb', 0),
                    'modified': download_info['info'].get('modified', '')
                }
            else:
                export_info['files'][file_type] = {'available': False}
        
        return export_info